# ğŸ“„ Multiple PDF Q&A Chat (RAG)

Welcome to the **Multiple PDF Q&A Chat (RAG)** project â€” an AI-powered app that enables you to **ask questions across multiple PDF documents**. Powered by **Retrieval-Augmented Generation (RAG)**, this project processes PDFs into searchable chunks and returns accurate, context-aware responses to your queries.

---
![image](https://github.com/user-attachments/assets/509c4f6a-d8ad-40ab-bdfc-cb634273fa46)

---

##  Key Features
 **Multi-PDF Support** â€” Upload one or more PDFs at once.  
 **Contextual Answers** â€” Answers grounded in the contents of your PDFs.  
 **Retrieval-Augmented Generation (RAG)** â€” Combines vector search with large language models for enhanced accuracy.  
 **User-Friendly UI** â€” Built with Streamlit for an interactive chat interface.  
 **Fast Semantic Search** â€” Powered by FAISS for scalable vector similarity lookup.

---

##  How It Works
1.  **Extract Text from PDFs** â€” PDFs are read and split into manageable chunks.
2.  **Create Embeddings** â€” Text chunks are transformed into vector embeddings using state-of-the-art models.
3. **Indexing with FAISS** â€” A FAISS vector index is created to support semantic search.
4.  **Query Processing** â€” When you ask a question, the most relevant chunks are retrieved.
5.  **Generate Answer** â€” The retrieved chunks are passed to an LLM to produce a context-aware answer.

---

## ğŸ› ï¸ Tech Stack
- **Language** â€” Python 3.x
- **Framework** â€” Streamlit
- **Core Libraries** â€” LangChain, PyPDF2, HuggingFace Transformers
- **Embeddings** â€” `jina-embeddings` or your preferred embedding model
- **Vector Database** â€” FAISS
- **LLM Backend** â€” Local or API-driven models (Groq)
  
---
